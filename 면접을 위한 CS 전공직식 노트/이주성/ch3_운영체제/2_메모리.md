## 메모리

### Dynamic RAM (DRAM)

![image.png](image1.png)

- DRAM은 데이터가 capacitor에 전하(charge)로 저장하고 Single transistor를 사용해 전하에 접근하는 휘발성 메모리이다.

* capacitor의 전하는 시간이 지나면서 자연적으로 방전되므로 주기적으로 데이터를 새로고침 해야한다.
  * 새로고침은 DRAM의 특정 row 단위로 수행되며 데이터를 읽고 다시 쓰는 방식으로 이뤄진다.
* 높은 저장 밀도를 가지며 비용 효율적으로 대량의 데이터를 저장할 수 있다.

**DRAM의 구조와 동작**

* DRAM의 비트는 직사각형 배열로 구성되어 있으며 각 비트는 특정 행과 열의 교차점에 위치한다.
* DRAM은 한 번에 하나의 행을 액세스한다.
  * 한 번에 여러 비트를 읽어올 수 있다.
* Burst mode: 한 번에 여러 단어를 연속적으로 공급해 latency를 줄이는 방식

![image.png](image2.png)

1. SDR (Single Data Rate)  DRAM
   * 클록 사이클 당 한 번의 데이터 전송 수행
2. DDR (Double Data Rate)  DRAM
   * 클록 사이클의 상승 엣지와 하강 엣지 모두에서 데이터를 전송해 속도를 2배로 증가시킴
   * DDR1, DDR2 … 계속 버전업
3. QDR (Quad Data Rate)  DRAM
   * DDR의 입출력을 분리해 클록 사이클 당 4번의 데이터 전송
   * 고성능 네트워킹 장비에 쓰임

**DRAM vs SRAM**

* DRAM (Dynamic RAM)은 주기적인 새로고침이 필요하지만, 저장 밀도가 높고 비용이 저렴해서 주로 시스템 메모리로 사용된다.
* SRAM (Static RAM)은 새로고침이 필요 없지만, 저장 밀도가 낮고 비용이 비싸다. 주로 캐시 메모리로 사용된다.

<br/>

---

### 메모리 계층

![image.png](image.png)


* 레지스터
  * CPU에 직접 붙어 있으며 가장 빠르고 용량이 작음
  * 휘발성 메모리
* 캐시
  * CPU에 가까이 위치하며 L1, L2, ... 캐시 존재
  * 휘발성 메모리
* 메인 메모리
  * 시스템의 RAM
  * primary storage
  * 휘발성 메모리
* Flash Disk
  * SSD와 같은 비휘발성 메모리
  * secondary storage
* Traditional Disk
  * HDD와 같은 비휘발성 메모리
* Remote Secondary Storage
  * 인터넷 등의 원격 저장소
  * 매우 느림

<br/>

---

### Locality(지역성)의 원리

**Temporal Locality (시간 지역성)**

* 더 최근에 접근한 데이터를 프로세서에 더 가까운 메모리에 저장해 빠르게 접근하도록 한다.

**Spetial Locality (공간 지역성)**

* 연속된 메모리 단어로 구성된 블록을 더 높은 계층의 메모리로 이동시켜 속도를 향상시킨다.


**Locality의 장점 - 메모리 계층 구조 구현**

* Locality를 메모리의 hierarchy를 만들 때 사용한다.
* 다양한 속도와 크기를 가진 여러 레벨의 메모리로 구성
  * 속도가 빠를수록 비용이 더 많이 들고 크기가 작아진다.
* multiple하게 만들 것
  * register - cache - main memory - disk

<br/>

---

### 캐시 히트와 캐시 미스

![image.png](image3.png)

**데이터 접근 시나리오**

1. 데이터가 상위 레벨(ex: 캐시, 레지스터)에 있는 경우 (Hit)
    - Hit Ratio
        - 데이터 접근 시 성공적으로 상위 레벨에서 데이터를 찾는 비율
        - 전체 접근 횟수 중 Hit 횟수

2. 데이터가 상위 레벨에 없는 경우 (Miss)
    - 하위 레벨에서 데이터를 가져와야 한다.
    - 레지스터의 경우(상위 레벨) 필요하면 그냥 접근하면 되지만 메모리(하위 레벨)는 넓어서 먼저 필요한 데이터 보유 여부를 알아야 한다.
    - Miss Penalty
        - 하위 레벨에서 데이터를 가져와 상위 레벨의 블록을 교체하는 데 걸리는 시간
            - 위아래 블록을 교체할 때에 copy 작업이 필요하므로 오버헤드가 발생
        - 데이터 전송과 블록 교체를 포함한 시간
    - Miss Ratio
        - Miss가 발생해 하위 레벨에서 데이터를 가져와야 하는 비율
        - 전체 접근 횟수 중 Miss 횟수


<br/>

---

### 캐시 매핑

- CPU의 레지스터와 메인 메모리(RAM) 간 데이터를 주고받을 때 캐시가 Hit되기 위해 매핑하는 방법 3가지

### 방법 1) Directed Mapping

![img_5.png](img_5.png)

- 주소를 어떻게 하면 더 빨리 찾을 수 있을까에 초점을 맞춰 캐시 설계
- 위치는 주소에 의해 결정되는데 하나의 블록은 캐시에서 정확히 한 위치에만 저장될 수 있다.
    - 계산 방식: (Block address) modulo (#Blocks in cache)
    - Block address가 00101이면, 캐시의 블록 수가 8인 경우
      → 00101 mudulo 8(01000) = 101 → 101 위치에 저장
    - 101은 00101 뿐만 아니라 10101, 20101도 다 사용한다.
- Tag를 사용해 블록 주소를 저장해 정확한 위치를 식별한다. 
  - 101의 경우 00101인지 10101인지 알 수 있도록 태그 표시
- Valid bit (유효 비트)를 사용해 유효한 데이터인지 확인 (초기에는 다 0)
- 처리가 빠르지만 충돌 발생이 잦다.

**캐시 메모리 접근 과정**
1. Address(메인 메모리의 특정 위치)에 대해 Modulo(나머지) 연산을 통해 캐시 인덱스로 변환
2. 캐시 인덱스로 Block에 Access
3. Valid Bit를 확인해 접근한 캐시 Block의 데이터가 유효한지 확인 
   - Valid Bit이 유효하다면(1) Hit!!
     - Block Address의 Tag 부분을 확인해 현재 접근한 블록이 실제로 필요한 데이터인지 확인
   - Valid Bit이 유효하지 않다면(0) Miss!!
     - 메인 메모리에서 데이터를 가져와 캐시 Block에 저장하고 Valid Bit를 1로 설정
4. Data Access

### 방법 2) Associative Mapping

- 순서를 일치시키지 않고 관련 있는 캐시와 메모리를 매핑
- 블록이 캐시의 어느 위치에든 저장될 수 있기 때문에 충돌이 적지만 모든 블록을 검색해야 하므로 Directed Mapping보다는 느릴 수 있다.
- Tag를 사용해 메모리 주소와 일치하는지 확인
- Valid Bit를 통해 블록이 유효한지 확인

**캐시 메모리 접근 과정**

1. 메인 메모리의 특정 Address(주소)에 대해 모든 캐시 블록에 접근할 수 있다.
2. 모든 캐시 블록의 Tag를 순차적으로 검사하여, 해당 주소와 일치하는 블록을 찾는다.
3. Valid Bit을 확인해 접근한 블록이 유효한지 확인
   - Valid Bit이 유효하다면(1) Hit!!
     - 일치하는 블록의 데이터를 사용한다. 
   - Valid Bit이 유효하지 않다면(0) Miss!!
     - 메인 메모리에서 데이터를 가져와 캐시 블록에 저장하고 Valid Bit를 1로 설정
4. Data Access


![img_6.png](img_6.png)

**Set Associative**
- 각 집합에 여러 엔트리가 있어서 같은 집합 내에서 여러 위치에 저장될 수 있어 유연성 높음
- 블록은 n개의 가능한 위치 중 하나에 저장될 수 있음
- n이 증가할수록 열이 늘어나고 행이 줄어든다.
    - n이 증가할수록 캐시 내 블록을 저장할 수 있는 위치의 유연성이 커지며 그로 인해 Miss rate가 감소하지만 Hit time은 증가하게 됨 → 좋지만 그만큼 비용 증가!

**Fully associative**
- 블록이 캐시 내 어느 위치에는 저장될 수 있음
- 가장 유연하지만 비용이 많이 든다.
- fully의 경우 하나의 행



### 방법 3) Set Associative Mapping

- Directed Mapping과 Associative Mapping의 절충안으로, 캐시 블록들을 여러 개의 그룹(Set)으로 분리
- 각각의 메모리 블록은 특정 Set 내의 모든 블록 중 어느 위치에든 저장될 수 있음
- 블록 주소를 통해 특정 Set을 결정하고, 해당 Set 내에서만 블록이 저장될 수 있음
- 더 많은 Set을 두면 탐색 범위가 줄어들고, 성능이 향상된다. 
  - Set의 크기가 클수록 유연성이 높아지지만, 탐색해야 할 블록 수가 많아진다.

**캐시 메모리 접근 과정**
1. 메인 메모리의 특정 Address에 대해 Modulo 연산을 통해 캐시 Set 인덱스로 변환
2. 해당 Set 내의 모든 블록을 검사하여 메모리 주소와 일치하는지 확인
3. Valid Bit을 확인해 접근한 블록이 유효한지 확인
   - Valid Bit이 유효하다면(1) Hit!!
     - 일치하는 블록의 데이터를 사용한다.
   - Valid Bit이 유효하지 않다면(0) Miss!!
     - 메인 메모리에서 데이터를 가져와 Set 내의 빈 블록에 저장하고 Valid Bit를 1로 설정
4. Data Access


<br/>

---
### 웹 브라우저의 캐시

**쿠키**
- 만료 기한이 있는 key-value 저장소
- same site 옵션을 strict로 설정하지 않았을 경우 다른 도메인에서 요청했을 때 자동 전송
- document.cookie로 쿠키를 볼 수 없게 httponly 옵션을 설정하는 것이 중요

**로컬 스토리지**
- 만료 기한이 없는 key-value 저장소
- 웹 브라우저를 닫아도 유지

**세션 스토리지**
- 만료 기한이 없는 key-value 저장소
- 탭 단위로 세션 스토리지 생성
- 탭을 닫으면 해당 데이터 삭제


<br/>

---

### 메모리 관리

**가상 메모리(Virtual Memory)**

컴퓨터가 실제로 이용 가능한 메모리 자원을 추상화해 사용하는 사용자들에게 매우 큰 메모리로 보이게 만드는 메모리 관리 기법

![img_8.png](img_8.png)

- **MMU (Memory Management Unit)**
    - Virtual Memory Space와 실제 Physical Space를 MMU가 Paging을 기반으로 매핑한다.
    - MMU는 CPU로부터 받은 가상 주소(VAs)를 물리 주소(Physical Address, PAs)로 변환한다.
        - Page Map 또는 Page Table을 사용해 VAs를 PAs로 변환한다.
        - 각 VAs는 물리 페이지에 매핑된다.
    - PAs는 메모리 접근을 효율적으로 관리하고, 프로그램 간 메모리 충돌을 방지하기 위해 사용된다.
    - 물리 메모리가 부족할 때 사용되지 않는 페이지를 하드 드라이브의 Swap 영역으로 보내 공간 확보


- **DRAM (Dynamic Random Access Memory)**
    - DRAM은 컴퓨터의 주 메모리로 사용되며, CPU는 이를 통해 데이터를 신속하게 읽고 쓸 수 있다.


- **디스크**
    - 시스템의 영구 저장소로, 물리 메모리보다 훨씬 느리지만 더 많은 양의 데이터를 저장할 수 있다.
    - DRAM이 가득 찼을 때는, 메모리의 일부 데이터를 디스크로 스왑아웃(swap out)하여 공간을 확보한다.


- **TLB(Translation Lookaside Buffer)**
    - 가장 최근에 참조된 주소 변환을 캐시하는 작은 메모리
    - MMU가 물리 메모리 전체를 검색하지 않고도 빠르게 주소 변환을 수행할 수 있어 속도를 크게 향상시킨다.


**Memory Virtualization 적용 → Tow-level Translaction이 필요**
  1. **Guest VA → Guest PA (First-Level Translation) (가상 머신 내부에서 수행)**
      - VM은 자체적인 가상 메모리를 관리하며, 게스트 OS 내에서 VAs를 자신의 PAs로 변환한다.
      - 게스트 PAs는 게스트 OS의 관점에서 PAs이지만, 실제로는 호스트 시스템에 의해 할당된 메모리 공간을 나타낸다.
          - 이 주소 공간은 호스트의 메모리에서 분리된 영역
  2. **Guest PA → Host PA (Second-Level Translation) (하이퍼바이저에서 수행)**
      - Guest PA = Host VA
      - VM은 게스트 PA를 호스트 시스템의 실제 물리 메모리 주소로 다시 변환한다.


**스와핑**
가상 메모리에는 존재하지만 실제 메모리인 RAM에는 현재 없는 데이터일 경우 페이지 폴트가 발생하는데 이 때 메모리에서 당장 사용하지 않는 영역을 하드디스크로 옮기고 하드디스크의 일부분을 마치 메모리처럼 불러와 쓰는 것을 말한다.

1. 페이지 폴트 발생 -> 트랩 발생 
2. 운영체제는 실제 디스크에서 사용하지 않는 프레임 탐색
3. 해당 프레임을 실제 메모리에 가져와서 페이지 교체 알고리즘을 기반으로 특정 페이지와 교체 (스와핑)
4. 페이지 테이블을 갱신시킨 후 해당 명령어 다시 시작


**스레싱**
메모리에 너무 많은 프로세스가 동시에 올라가 스와핑이 많이 일어나면서 메모리의 페이지 폴트율이 높아져 발생한다.

1. 페이지 폴트가 일어나면서 CPU 이용률 하락
2. CPU 이용률이 낮아지면 운영체제가 CPU가 일이 적다고 판단해 더 많은 프로세스를 메모리에 올림
3. 페이지 폴트가 더 많아지면서 악순환


- 해결 방법
  - 메모리 늘리기
  - HDD를 SSD로 바꾸기
  - 작업 세트 (working set): 시간 지역성을 통해 결정된 페이지 집합을 만들어서 미리 메로리에 로드해 스와핑 줄이기
  - PFF(Page Fault Frequency): 페이지 폴트의 상한선과 하한선을 설정해 프레임 조절


<br/>

---

### 메모리 할당

**연속 할당**
메모리에 연속적으로 공간을 할당하는 것으로 2가지 방식이 있다. 

1. 고정 분할 방식
   - 메모리를 미리 나눠 관리하기 때문에 유연하지 않다.
   - 메모리를 나눈 크기보다 프로그램이 작아서 들어가지 못하는 공간이 많이 발생하는 내부 단편화 현상 발생 

2. 가변 분할 방식
   - 매 시점 프로그램의 크기에 맞게 동적으로 메모리를 나눠 사용
   - 메모리를 나눈 크기보다 프로그램이 커서 들어가지 못하는 공간이 많이 발생하는 외부 단편화 현상 발생 가능
   - 3가지 방식
     - 최초 적합: 위쪽이나 아래쪽부터 시작해서 홀(할당할 수 있는 비어 있는 메모리 공간)을 찾으면 바로 할당
     - 최적 적합: 프로세스의 크기 이상인 공간 중 가장 작은 홀부터 할당
     - 최악 적합: 프로세스의 크기와 가장 많이 차이가 나는 홀에 할당

**불연속 할당**
현대 운영체제가 사용하는 방법으로 메모리를 동일한 크키의 페이지로 나누고 프로그램마다 페이지 테이블을 둬 메모리에 프로그램을 할당하는 방법이다.

- 페이징
    - 동일한 크기의 페이지로 단위로 나눠 서로 다른 위치에 프로세스를 할당
    - 홀 크기가 균일해서 좋지만 주소 변환이 복잡

- 세그멘테이션
    - 의미 단위인 세그먼트로 나누는 방식
    - 보안 측면에서 좋지만 홀 크기가 균일하지 않은 단점

- 페이지드 세그멘테이션
  - 의미 단위인 세그먼트로 나눠 공유나 보안 측면에 강점을 두는 방식
  - 동일한 크키의 페이지 단위로 나눔


<br/>

---

### 페이지 교체 알고리즘
메모리는 한정되어 있기 때문에 스와핑이 많이 일어나는데 이 때 페이지 교체 알고리즘이 사용된다.

**오프라인 알고리즘**
- 먼 미래에 참조되는 페이지 <-> 현재 페이지
- 가장 좋은 방법이지만 미래를 알 수 없기 때문에 불가능
- 비교 상한 기준 제공

**FIFO(First In First Out) 알고리즘**
- 가장 먼저 온 페이지를 교체 영역에 가장 먼저 놓는 알고리즘

**LRU(Least Recently Used) 알고리즘**
- 참조가 가장 오래된 페이지를 변경하는 알고리즘
- 오래된 것을 찾기 위해 각 페이지마다 계수기, 스택을 둬야 하는 문제
- 이중 연결 리스트로 한정된 메모리를 나타내고, 이 이중 연결 리스트에서 빠르게 찾을 수 있도록 해시 테이블을 사용한다.

**NUR(Not Used Recently) 알고리즘**
- clock 알고리즘이라고도 불리는 NUR 알고리즘은 0과 1을 가진 비트를 두고 시계 방향으로 돌면서 0을 찾아 해당 프로세스를 교체한 후 1로 바꾸는 알고리즘이다.
- 이 때 0은 최근에 참조되지 않았음을, 1은 최근에 참조되었음을 나타낸다. 

**LFU(Least Frequently Used 알고리즘**
- 가장 참조 횟수가 적은 페이지를 교체


<br/>
